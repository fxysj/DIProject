å½“ç„¶å¯ä»¥ï¼ä»¥ä¸‹æ˜¯ç”¨ Markdown (`.md`) æ ¼å¼æ•´ç†çš„äº”é“é¢˜ç›®ï¼Œæ¯é¢˜åŒ…æ‹¬èƒŒæ™¯æè¿°ã€ä»£ç ç¤ºä¾‹ï¼Œä»¥åŠæ¶µç›–ä½ æå‡ºçš„æ‰€æœ‰æŠ€æœ¯ç‚¹ã€‚

---

# ğŸ”¬ AI ç®—æ³•å¤§æ¨¡å‹å®æˆ˜é¢˜é›†

æ¶µç›–å¤§æ¨¡å‹å¾®è°ƒã€æœ¬åœ° BERT è¯­ä¹‰æ¨¡å‹è®­ç»ƒã€å¤šæ¨¡æ€æ¨¡å‹ã€LLM å·¥å…·é“¾ï¼ˆLLMFactoryã€VLLMã€Unslothï¼‰ã€HuggingFace Transformersï¼ˆå°¤å…¶ GPT-2ï¼‰ã€AI åˆ¶è¯ç­‰é‡ç‚¹æ–¹å‘ã€‚

---

## ğŸ§  é¢˜ç›®ä¸€ï¼šåŸºäº QLoRA çš„å¤§æ¨¡å‹å¾®è°ƒå®è·µ

### ğŸ“Œ ä»»åŠ¡æè¿°  
ä½¿ç”¨ `meta-llama/Llama-2-7b-hf` æ¨¡å‹ï¼Œç»“åˆ `QLoRA + PEFT` è¿›è¡Œä¸­æ–‡é—®ç­”å¾®è°ƒè®­ç»ƒï¼Œæ•°æ®é›†å¯é€‰ç”¨ Alpacaã€BELLE ç­‰ã€‚

### ğŸ’¡ æŠ€æœ¯ç‚¹
- LoRA å¾®è°ƒ
- PEFT æ¡†æ¶
- 8bit é‡åŒ–åŠ è½½

### ğŸ§ª ç¤ºä¾‹ä»£ç 
```python
from peft import get_peft_model, LoraConfig, TaskType
from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "meta-llama/Llama-2-7b-hf"
tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_name, load_in_8bit=True, device_map="auto")

peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8, lora_alpha=32, lora_dropout=0.1,
    bias="none"
)

model = get_peft_model(model, peft_config)
```

---

## ğŸ§¬ é¢˜ç›®äºŒï¼šåŸºäºæœ¬åœ° BERT çš„è¯­ä¹‰åŒ¹é…æ¨¡å‹

### ğŸ“Œ ä»»åŠ¡æè¿°  
ä½¿ç”¨ `bert-base-chinese` æ„å»º Siamese ç½‘ç»œç»“æ„ï¼Œå®Œæˆæ–‡æœ¬å¯¹ä¹‹é—´çš„è¯­ä¹‰ç›¸ä¼¼åº¦è®¡ç®—ä»»åŠ¡ã€‚

### ğŸ’¡ æŠ€æœ¯ç‚¹
- BERT ç¼–ç å™¨
- Cosine ç›¸ä¼¼åº¦
- Siamese åŒå¡”ç»“æ„

### ğŸ§ª ç¤ºä¾‹ä»£ç 
```python
from transformers import BertTokenizer, BertModel
import torch.nn as nn

class SiameseBERT(nn.Module):
    def __init__(self):
        super().__init__()
        self.bert = BertModel.from_pretrained("bert-base-chinese")
        self.cos = nn.CosineSimilarity(dim=1)

    def forward(self, input1, input2):
        out1 = self.bert(**input1).last_hidden_state[:, 0, :]
        out2 = self.bert(**input2).last_hidden_state[:, 0, :]
        return self.cos(out1, out2)
```

---

## ğŸ§‘â€ğŸ¨ é¢˜ç›®ä¸‰ï¼šå¤šæ¨¡æ€å›¾æ–‡åŒ¹é…æ¨ç†ç³»ç»Ÿ

### ğŸ“Œ ä»»åŠ¡æè¿°  
æ„å»ºä¸€ä¸ª CLIP é£æ ¼å›¾æ–‡å¯¹æ¯”æ¨¡å‹ï¼Œå®ç°å›¾æ–‡åŒ¹é…èƒ½åŠ›ï¼Œæ”¯æŒé¢„æµ‹å›¾ç‰‡å’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚

### ğŸ’¡ æŠ€æœ¯ç‚¹
- OpenCLIP æ¨¡å‹ç»“æ„
- å›¾æ–‡å‘é‡ç¼–ç å¯¹æ¯”
- softmax ç›¸ä¼¼åº¦æ¨ç†

### ğŸ§ª ç¤ºä¾‹ä»£ç 
```python
from transformers import CLIPProcessor, CLIPModel
import torch

model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")

inputs = processor(text=["ä¸€åªçŒ«"], images=["cat.jpg"], return_tensors="pt", padding=True)
outputs = model(**inputs)
logits_per_image = outputs.logits_per_image
probs = logits_per_image.softmax(dim=1)
```

---

## âš™ï¸ é¢˜ç›®å››ï¼šåŸºäº LLMFactory + VLLM + Unsloth çš„é«˜æ€§èƒ½æ¨ç†éƒ¨ç½²

### ğŸ“Œ ä»»åŠ¡æè¿°  
åŸºäº `LLMFactory` æ„å»ºé«˜å¹¶å‘æ¨ç†æœåŠ¡ï¼Œåº•å±‚ä½¿ç”¨ `VLLM` åšæ¨ç†å¼•æ“ï¼Œæ¨¡å‹æ¥è‡ª `Unsloth` é‡åŒ–åçš„è½»é‡ LLMã€‚

### ğŸ’¡ æŠ€æœ¯ç‚¹
- VLLM æ¨ç†åŠ é€Ÿ
- Unsloth è½»é‡æ¨¡å‹
- LLMFactory åŠ¨æ€æœåŠ¡ç¼–æ’

### ğŸ§ª ç¤ºä¾‹ä»£ç 
```python
from llmfactory import LLMServer
from vllm import LLM, SamplingParams
import torch

server = LLMServer()
model = LLM(model="unsloth/llama-2-7b-qlora", dtype=torch.float16)
sampling_params = SamplingParams(temperature=0.7, top_p=0.9)

def handle_prompt(prompt: str):
    return model.generate(prompt, sampling_params=sampling_params)

server.register_model("chat-agent", handle_prompt)
server.serve()
```

---

## ğŸ’Š é¢˜ç›®äº”ï¼šåŸºäº GPT-2 çš„åˆ†å­è¯­è¨€æ¨¡å‹ + AI è¯ç‰©ç”Ÿæˆ

### ğŸ“Œ ä»»åŠ¡æè¿°  
ä½¿ç”¨ `GPT-2` è®­ç»ƒç”Ÿæˆ SMILES åŒ–å­¦åˆ†å­è¯­è¨€æ¨¡å‹ï¼Œç”ŸæˆåŒ–åˆç‰©ç»“æ„è¯´æ˜ï¼ŒåŒæ—¶ç»“åˆç‰¹å¾å·¥ç¨‹ç”¨äº QSAR è¯æ•ˆé¢„æµ‹ã€‚

### ğŸ’¡ æŠ€æœ¯ç‚¹
- HuggingFace GPT-2 æ–‡æœ¬ç”Ÿæˆ
- SMILES åŒ–å­¦åºåˆ—
- è¯ç‰©å±æ€§ç”Ÿæˆä»»åŠ¡

### ğŸ§ª ç¤ºä¾‹ä»£ç 
```python
from transformers import GPT2LMHeadModel, GPT2Tokenizer, TextDataset, DataCollatorForLanguageModeling, Trainer, TrainingArguments

tokenizer = GPT2Tokenizer.from_pretrained("gpt2")
model = GPT2LMHeadModel.from_pretrained("gpt2")

train_dataset = TextDataset(tokenizer=tokenizer, file_path="drug_smiles.txt", block_size=128)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)

training_args = TrainingArguments(
    output_dir="./drug-gpt2", overwrite_output_dir=True,
    per_device_train_batch_size=4, num_train_epochs=3
)

trainer = Trainer(model=model, args=training_args,
                  data_collator=data_collator, train_dataset=train_dataset)

trainer.train()
```

---