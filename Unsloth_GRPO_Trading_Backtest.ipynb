
# 🔧 安装依赖
!pip install unsloth datasets transformers accelerate peft bitsandbytes -q
!pip install git+https://github.com/huggingface/trl.git -q

# 🌐 导入模块
from datasets import load_dataset
from transformers import AutoTokenizer
from unsloth import FastLanguageModel
from trl import AutoModelForCausalLMWithValueHead, PPOTrainer, PPOConfig, create_reference_model
from peft import prepare_model_for_kbit_training
import torch
import random

# ✅ 加载数据集
dataset = load_dataset("0xscope/web3-trading-analysis", split="train")
dataset = dataset.shuffle(seed=42).select(range(1000))  # 仅取部分数据用于演示

# 🧠 Prompt 构建函数
def format_prompt(example):
    return {
        "prompt": f"""
你是一名资深的区块链量化策略分析师。

以下是某笔交易信息：
- 链：{example['chain']}
- 代币地址：{example['token_address']}
- 事件类型：{example['event_type']}
- 事件详情：{example['event']}

请基于以上信息，结合历史数据趋势、频繁交易行为、合约类型等特征，预测该资产未来3天的市场表现，给出建议（如买入/卖出/持有）并给出具体策略理由。
""",
        "response": example["strategy_recommendation"] if "strategy_recommendation" in example else "建议：持有。理由：暂无重大变动，建议观望。",
    }

dataset = dataset.map(format_prompt)

# ✂️ 清洗字段
dataset = dataset.remove_columns([col for col in dataset.column_names if col not in ["prompt", "response"]])

# 🧠 加载模型（使用 Unsloth + QLoRA）
model_name = "unsloth/llama-3-8b-Instruct-bnb-4bit"
max_seq_length = 2048
dtype = torch.float16

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing=False,
    random_state=42,
)

tokenizer.pad_token = tokenizer.eos_token

# 🔁 构造强化学习数据
ppo_dataset = [{"query": ex["prompt"], "response": ex["response"]} for ex in dataset]

# 🧮 奖励函数（策略回测评分）
def reward_fn(response):
    if "买" in response:
        return 0.8
    elif "卖" in response:
        return 0.6
    elif "持有" in response:
        return 0.4
    else:
        return 0.1

# ⚙️ PPO 配置
ppo_config = PPOConfig(
    model_name=model_name,
    learning_rate=5e-6,
    batch_size=1,
    mini_batch_size=1,
    gradient_accumulation_steps=4,
    optimize_device_cache=True,
)

ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model)
ref_model = create_reference_model(ppo_model)

ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=ppo_model,
    ref_model=ref_model,
    tokenizer=tokenizer,
)

# 🚀 训练循环（每一轮根据 prompt 生成响应，然后奖励训练）
for epoch, sample in enumerate(ppo_dataset[:10]):
    query = sample["query"]
    response = sample["response"]

    reward = reward_fn(response)
    ppo_trainer.step([query], [response], [reward])
    print(f"✅ Epoch {epoch + 1} completed. Reward: {reward}")
