
# ğŸ”§ å®‰è£…ä¾èµ–
!pip install unsloth datasets transformers accelerate peft bitsandbytes -q
!pip install git+https://github.com/huggingface/trl.git -q

# ğŸŒ å¯¼å…¥æ¨¡å—
from datasets import load_dataset
from transformers import AutoTokenizer
from unsloth import FastLanguageModel
from trl import AutoModelForCausalLMWithValueHead, PPOTrainer, PPOConfig, create_reference_model
from peft import prepare_model_for_kbit_training
import torch
import random

# âœ… åŠ è½½æ•°æ®é›†
dataset = load_dataset("0xscope/web3-trading-analysis", split="train")
dataset = dataset.shuffle(seed=42).select(range(1000))  # ä»…å–éƒ¨åˆ†æ•°æ®ç”¨äºæ¼”ç¤º

# ğŸ§  Prompt æ„å»ºå‡½æ•°
def format_prompt(example):
    return {
        "prompt": f"""
ä½ æ˜¯ä¸€åèµ„æ·±çš„åŒºå—é“¾é‡åŒ–ç­–ç•¥åˆ†æå¸ˆã€‚

ä»¥ä¸‹æ˜¯æŸç¬”äº¤æ˜“ä¿¡æ¯ï¼š
- é“¾ï¼š{example['chain']}
- ä»£å¸åœ°å€ï¼š{example['token_address']}
- äº‹ä»¶ç±»å‹ï¼š{example['event_type']}
- äº‹ä»¶è¯¦æƒ…ï¼š{example['event']}

è¯·åŸºäºä»¥ä¸Šä¿¡æ¯ï¼Œç»“åˆå†å²æ•°æ®è¶‹åŠ¿ã€é¢‘ç¹äº¤æ˜“è¡Œä¸ºã€åˆçº¦ç±»å‹ç­‰ç‰¹å¾ï¼Œé¢„æµ‹è¯¥èµ„äº§æœªæ¥3å¤©çš„å¸‚åœºè¡¨ç°ï¼Œç»™å‡ºå»ºè®®ï¼ˆå¦‚ä¹°å…¥/å–å‡º/æŒæœ‰ï¼‰å¹¶ç»™å‡ºå…·ä½“ç­–ç•¥ç†ç”±ã€‚
""",
        "response": example["strategy_recommendation"] if "strategy_recommendation" in example else "å»ºè®®ï¼šæŒæœ‰ã€‚ç†ç”±ï¼šæš‚æ— é‡å¤§å˜åŠ¨ï¼Œå»ºè®®è§‚æœ›ã€‚",
    }

dataset = dataset.map(format_prompt)

# âœ‚ï¸ æ¸…æ´—å­—æ®µ
dataset = dataset.remove_columns([col for col in dataset.column_names if col not in ["prompt", "response"]])

# ğŸ§  åŠ è½½æ¨¡å‹ï¼ˆä½¿ç”¨ Unsloth + QLoRAï¼‰
model_name = "unsloth/llama-3-8b-Instruct-bnb-4bit"
max_seq_length = 2048
dtype = torch.float16

model, tokenizer = FastLanguageModel.from_pretrained(
    model_name=model_name,
    max_seq_length=max_seq_length,
    dtype=dtype,
    load_in_4bit=True,
)

model = FastLanguageModel.get_peft_model(
    model,
    r=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
    lora_alpha=16,
    lora_dropout=0,
    bias="none",
    use_gradient_checkpointing=False,
    random_state=42,
)

tokenizer.pad_token = tokenizer.eos_token

# ğŸ” æ„é€ å¼ºåŒ–å­¦ä¹ æ•°æ®
ppo_dataset = [{"query": ex["prompt"], "response": ex["response"]} for ex in dataset]

# ğŸ§® å¥–åŠ±å‡½æ•°ï¼ˆç­–ç•¥å›æµ‹è¯„åˆ†ï¼‰
def reward_fn(response):
    if "ä¹°" in response:
        return 0.8
    elif "å–" in response:
        return 0.6
    elif "æŒæœ‰" in response:
        return 0.4
    else:
        return 0.1

# âš™ï¸ PPO é…ç½®
ppo_config = PPOConfig(
    model_name=model_name,
    learning_rate=5e-6,
    batch_size=1,
    mini_batch_size=1,
    gradient_accumulation_steps=4,
    optimize_device_cache=True,
)

ppo_model = AutoModelForCausalLMWithValueHead.from_pretrained(model)
ref_model = create_reference_model(ppo_model)

ppo_trainer = PPOTrainer(
    config=ppo_config,
    model=ppo_model,
    ref_model=ref_model,
    tokenizer=tokenizer,
)

# ğŸš€ è®­ç»ƒå¾ªç¯ï¼ˆæ¯ä¸€è½®æ ¹æ® prompt ç”Ÿæˆå“åº”ï¼Œç„¶åå¥–åŠ±è®­ç»ƒï¼‰
for epoch, sample in enumerate(ppo_dataset[:10]):
    query = sample["query"]
    response = sample["response"]

    reward = reward_fn(response)
    ppo_trainer.step([query], [response], [reward])
    print(f"âœ… Epoch {epoch + 1} completed. Reward: {reward}")
