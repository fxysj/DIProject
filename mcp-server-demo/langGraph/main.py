import os
from dotenv import load_dotenv
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, END
from typing import TypedDict, Optional

from tool_registry import registry  # ä½ è¦è‡ªå·±æä¾› registry å·¥å…·æ³¨å†Œæ¨¡å—

load_dotenv()


class AgentState(TypedDict):
    query: str
    intent: Optional[str]
    result: Optional[str]


# å®šä¹‰å¯ç”¨å·¥å…·åŠæ„å›¾
available_tools = {
    "calculate_bmi": "è®¡ç®—BMI",
    "translate_text": "ç¿»è¯‘æ–‡æœ¬",
    "generate_image": "ç”Ÿæˆå›¾ç‰‡",
}

tool_list_text = "\n".join([f"- {k}ï¼š{v}" for k, v in available_tools.items()])

# æ„å›¾è¯†åˆ« Prompt
intent_prompt = ChatPromptTemplate.from_template(f"""
ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½æ„å›¾è¯†åˆ«åŠ©æ‰‹ã€‚è¯·æ ¹æ®ç”¨æˆ·çš„æé—®å†…å®¹ï¼Œä»ä»¥ä¸‹æ„å›¾ä¸­é€‰æ‹©ä¸€ä¸ªæœ€åŒ¹é…çš„ï¼š
{tool_list_text}
ç”¨æˆ·è¯´ï¼š{{query}}
è¯·åªè¿”å›æ„å›¾å…³é”®è¯ï¼ˆå¦‚ calculate_bmiï¼‰ï¼Œä¸è¦æ·»åŠ å…¶ä»–æ–‡å­—ã€‚
""")

# OpenAI æ¨¡å‹
llm = ChatOpenAI(
    model="gpt-4o",
    temperature=0.3,
    openai_api_key=os.getenv("OPENAI_API_KEY"),
    openai_api_base=os.getenv("OPENAI_BASE_URL")
)

# å®Œæ•´æ„å›¾é“¾æ¡
intent_chain = intent_prompt | llm | StrOutputParser()

# Intentè¯†åˆ«é€»è¾‘
def intent_parse(state: AgentState) -> AgentState:
    query = str(state["query"])
    print("ğŸ“¥ ç”¨æˆ·è¾“å…¥:", query)
    intent = intent_chain.invoke({"query": query}).strip()
    print("ğŸ” è¯†åˆ«æ„å›¾:", intent)
    return {**state, "intent": intent}

# MCPå·¥å…·æ‰§è¡Œé€»è¾‘
def run_mcp_tool(state: AgentState) -> AgentState:
    intent = state.get("intent")
    query = state.get("query")
    if intent in registry:
        tool = registry[intent]()
        result = tool.run(query)
        return {**state, "result": result}
    return {**state, "result": f"æœªçŸ¥æ„å›¾ï¼š{intent}"}

# åˆ†æ”¯è·¯ç”±
def router(state: AgentState) -> str:
    intent = state.get("intent", "")
    return "call_mcp" if intent in registry else "fallback"

# å…œåº•å¤„ç†
def fallback_handler(state: AgentState) -> AgentState:
    return {**state, "result": "âŒ æ— æ³•è¯†åˆ«æ„å›¾æˆ–å·¥å…·æœªå®ç°ã€‚"}

# æ„å»ºLangGraph
graph = StateGraph(AgentState)
graph.add_node("detect_intent", intent_parse)
graph.add_node("call_mcp", RunnableLambda(run_mcp_tool))
graph.add_node("fallback", RunnableLambda(fallback_handler))

graph.set_entry_point("detect_intent")
graph.add_conditional_edges("detect_intent", router)
graph.set_finish_point("call_mcp")
graph.set_finish_point("fallback")

workflow = graph.compile()


async def run():
    query_text = "è¯·å¸®æˆ‘è®¡ç®—ä¸€ä¸‹ BMIï¼Œä½“é‡70å…¬æ–¤ï¼Œèº«é«˜1ç±³75"
    print("ğŸ§ª å¼€å§‹æµ‹è¯•ï¼š", query_text)
    state = {"query": query_text}
    result = await workflow.ainvoke(state)
    print("âœ… æœ€ç»ˆç»“æœ:", result)


# è¿è¡Œæµ‹è¯•
import asyncio

if __name__ == '__main__':
    asyncio.run(run())
